---
title: "An underbaked GPT-Codex-5.3-Spark review"
date: 2026-02-14
draft: true
---
Two days ago, OpenAI released [GPT-5.3-Codex-Spark](https://openai.com/index/introducing-gpt-5-3-codex-spark/). I begrudgingly subscribed to ChatGPT Pro to try it, and now that I've used it for two days, I have a comprehensive understanding of the model and am ready to pass judgement. 

The speed of the model is *incredible*. Watching tokens fly by at breakneck speed, especially when it's purely outputting tokens (as opposed to searching or using subagents, which doesn't show up in the UI), is magical. 

It's also the first time I've seriously used the [Codex Mac app](https://openai.com/codex) or their CLI, both of which are on-par with what's expected these days (and the Mac app has a nice way of seeing diffs, both from the current turn and the whole branch diff against `main`). 

However, my experience with `gpt-5.3-codex-spark` is that it's *noticeably* dumber. I was initially very impressed with a couple tasks, in which it got the answer correct and very fast. But 